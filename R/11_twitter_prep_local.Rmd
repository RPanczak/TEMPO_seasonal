---
title: "Twitter - data preparation"
author: "Radoslaw Panczak"
date: "`r format(Sys.time(), '%d %B, %Y')`"
mainfont: DejaVu Sans
output: 
  html_document: 
    # css: custom.css
    toc: true
    toc_depth: 4
    toc_float: true
    number_sections: true
    theme: united 
    highlight: pygments 
editor_options: 
  chunk_output_type: console
---

<!-- ------------------------------------------------------------ --> 
<!-- ------------------------------------------------------------ --> 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      fig.width=9, fig.height=7, dpi=300, out.width="900px", out.height="700px")

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

options(scipen=999)
set.seed(12345)

library(pacman) 
p_load(tidyverse, magrittr, naniar, here, janitor, scales,
       anytime, lubridate, lutz, 
       skimr, sjmisc, kableExtra, 
       sf, tmap, tmaptools)

tmap_mode("view") # makes map interactive

isUnique <- function(vector){
  return(!any(duplicated(vector)))
}

# # State data
# STE <- st_transform(st_read("./data/geo/1270055001_ste_2016_aust_shape/STE_2016_AUST.shp"), 4326)
# STE$AREASQKM16 <- NULL
# # st_is_valid(STE)
# 
# # buffer 0.5 degrees prepared in ArcGIS
# STE_b_05 <- st_transform(st_read("./data/geo/1270055001_ste_2016_aust_shape/STE_2016_AUST_B05.shp"), 4326)
# # st_is_valid(STE_b_05)

```

<!-- ------------------------------------------------------------ --> 

```{r raw_data_update, eval=FALSE}
# Connection

# That chunk should not be normally evaluated and is used only occasionally to update data!

p_load(bigrquery)

project <- "goopla-1509504704864"

sql <- "SELECT * FROM `twitter-stream-205107.twitter_stream.streaming`"

tb <- bq_project_query(project, sql)

tweets_raw <- bq_table_download(tb, bigint = "integer64")

saveRDS(tweets_raw, file = "./data/Twitter/raw/tweets_raw.rds")

# tweets_raw_10k <- tweets_raw %>% 
#   sample_n(10000)
# saveRDS(tweets_raw_10k, file = "./data/raw/tweets_raw_10k.rds")

# saving to a bucket 
# bq_table_save(tb, "gs://twitter_stream-streaming/2018-12-04_*.csv")

# download chunks
# gsutil ls gs://twitter_stream-streaming/
# gsutil ls -l gs://twitter_stream-streaming/
# gsutil cp gs://twitter_stream-streaming/2018-12-04_*.csv R:\TWITTER-I0479\data\raw\2018-12-04\
# gsutil rm gs://twitter_stream-streaming/2018-12-04_*.csv


# contains place_full_name for the top 39,999 occurring places, tweet count in the original tweet table, and the full geocode JSON result from Google's Maps API

sql <- "SELECT * FROM `twitter-stream-205107.twitter_stream.twitter_places_geocoded`"

tb <- bq_project_query(project, sql)

twitter_places_geocoded <- bq_table_download(tb, bigint = "integer64")

saveRDS(twitter_places_geocoded, file = "./data/Twitter/raw/twitter_places_geocoded.rds")


# place_full_name, lat/lon reported by google, corresponding SA2 MAINCODE and the max spatial resolution. (Still working on the spatial resolution)

sql <- "SELECT * FROM `twitter-stream-205107.twitter_stream.geocoded_places_asgs`"

tb <- bq_project_query(project, sql)

geocoded_places_asgs <- bq_table_download(tb, bigint = "integer64")

saveRDS(geocoded_places_asgs, file = "./data/Twitter/raw/geocoded_places_asgs.rds")


# place_full_name, lat/lon reported by google, corresponding SA2 MAINCODE and the max spatial resolution. (Still working on the spatial resolution)

sql <- "SELECT * FROM `twitter-stream-205107.twitter_stream.twitter_ABS_month_SA3`"

tb <- bq_project_query(project, sql)

twitter_ABS_month_SA3 <- bq_table_download(tb)

saveRDS(twitter_ABS_month_SA3, file = "./data/Twitter/clean/twitter_ABS_month_SA3.rds")

rm(project, sql, tb)

p_unload(bigrquery)
```


<!-- ------------------------------------------------------------ --> 
# Data prep  

```{r}
tweets_clean_2019_07_22 <- 
  readRDS("./data/Twitter/clean/tweets_proc_2019_07_22.rds") %>%
  # readRDS("R:/POP247-Q0786/data/Twitter/clean/tweets_proc_2019_07_22.rds") %>% 
  select(-place_url, -user_geo_enabled, -user_verified, -lang) %>%
  filter(!is.na(created_at))

# sjmisc::frq(tweets_clean_2019_07_22, source, sort.frq = "desc")
# sjmisc::frq(tweets_clean_2019_07_22, user_geo_enabled, sort.frq = "desc")
# sjmisc::descr(tweets_clean_2019_07_22, lat)

```

Due to volume of data preprocessing was done on HPC using script `11_twitter_prep_hpc.R`. 

Preprocessed dataset has `r comma(nrow(tweets_clean_2019_07_22))` rows and `r comma(ncol(tweets_clean_2019_07_22))` columns

## IDs

### Fixing user id

Problem with `user_id_str` character in scientific notation (like `1.00031e+18`).

```{r, include=FALSE}
# length(unique(tweets_proc_2019_07_22$user_id_str))
# length(unique(tweets_proc_2019_07_22$user_screen_name))
# 
# id <- tweets_proc_2019_07_22 %>%
#   group_by(user_id_str) %>%
#   summarise(count = n()) %>%
#   arrange(desc(count))
# 
# name <- tweets_proc_2019_07_22 %>%
#   group_by(user_screen_name) %>%
#   summarise(count = n()) %>%
#   arrange(desc(count))

both <- tweets_clean_2019_07_22 %>%
  group_by(user_id_str, user_screen_name) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# dupli1 <- both %>%
#   group_by(user_screen_name) %>%
#   summarise(count = n()) %>%
#   filter(count > 1) %>%
#   arrange(desc(count))
# 
# dupli2 <- both %>%
#   group_by(user_id_str) %>%
#   summarise(count = n()) %>%
#   filter(count > 1) %>%
#   arrange(desc(count))
# 
# rm(dupli1, dupli2)
```

For example, same screen name & different ID:

```{r}
both %>% 
  filter(user_screen_name == "ausbiebersquad")
```

Or same ID & different screen name:

```{r}
both %>% 
  filter(user_id_str == "3281052492")

rm(both)
```

`user_id_str` & `user_screen_name` were combined in new variable `UID` that might be better for some applications (?).

### Fixing tweet id 

Similarly, because of scientific notation there is a problem with `id_str` character formatted like `1.00031e+18`.

```{r}
# isUnique(tweets_proc_2019_07_22$id_str)
# 
# length(tweets_proc_2019_07_22$id_str)
# length(unique(tweets_proc_2019_07_22$id_str))

tweets_clean_2019_07_22 %>% 
  select(id_str, created_at, place_full_name) %>% 
  filter(id_str == "1.00028e+18")
```

**Currently - no steps taken to resolve that.**

## Geography

### Poor spatial resolution

```{r}
# tweets_clean_2019_07_22 %>% 
#   filter(place_place_type == "country") %>% 
#   frq(place_full_name, sort.frq = "desc")
# 
# tweets_clean_2019_07_22 %>% 
#   filter(place_place_type == "admin") %>% 
#   frq(place_full_name, sort.frq = "desc")

temp <- tweets_clean_2019_07_22 %>% 
  filter((place_place_type == "country" | place_place_type == "admin") & is.na(lat)) 

# anyDuplicated(temp)
```

There are `r comma(nrow(temp))` tweets from `r comma(length(unique(temp$place_full_name)))` locations that have no geographic coordinates and `place_type` variable that indicates only country or large admin region (state). 

```{r}
temp %>% 
  group_by(place_full_name) %>% 
  summarise(freq = n()) %>% 
  arrange(desc(freq)) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

These tweets were removed from the data since little geographical analyses could be done here.

```{r}
temp %<>% 
  group_by(place_full_name) %>% 
  summarise(n = n()) %>% 
  select(-n)

tweets_clean_2019_07_22 %<>% anti_join(temp)
rm(temp)
```

### `geo` variable

Small amount of tweets have extra coordinates represented by `geo` variable. It has a format of `{"type":"Point","coordinates":[-35.23558731,138.2079786]}` and often coordinates are the same or very similar. Coordinates were extracted from that to `geo_lat` & `geo_lon`.

```{r}
geo1_sp <- readRDS(file = "./data/Twitter/clean/geo1_sp.rds")
```

There are `r sum(geo1_sp$freq)` tweets in `r nrow(geo1_sp)` locations having (more?) precise coordinates. 

```{r}
tm_shape(geo1_sp) + 
  tm_bubbles(size = "freq", col = "red")
```

Quite few of them turned out to be good candidates for deletions: 

```{r include=FALSE}
temp2 <- tweets_clean_2019_07_22 %>% 
  group_by(user_description) %>% 
  summarise(freq = n()) %>% 
  arrange(desc(freq)) %>% 
  ungroup()

tweets_clean_2019_07_22 %<>%
  filter(!grepl("semi automated bot", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("unofficial bot", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("Traffic alerts", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("LIVE Traffic", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("Follow this account for geo-targeted", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("Weather updates, forecast", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("Twitter trends", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("Hobart GPO Clock Tower", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("This is the #jobs feed", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("NSW RFS Incident feed", user_description, ignore.case = TRUE)) %>% 
  filter(!grepl("West Coast Sentinel", user_description, ignore.case = TRUE))

temp1 <- tweets_clean_2019_07_22 %>% 
  group_by(user_description) %>% 
  summarise(freq = n()) %>% 
  arrange(desc(freq)) %>% 
  ungroup()

temp3 <- anti_join(temp2, temp1)
```

`r comma(sum(temp3$freq))` tweets of `r comma(nrow(temp3))` bot-like users were excluded. Here are descriptions and frequencies: 

```{r}
temp3 %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

```{r include=FALSE}
rm(temp1, temp2, temp3, geo1_sp)
```

Remaining tweets with more precise coordinates were used to replace `lat` & `lon` values.

```{r}
# replacement already done in preproc; now only generating a flag
tweets_clean_2019_07_22 %<>% 
  mutate(sp_res = ifelse(!is.na(lat), "normal", "")) %>% 
  mutate(sp_res = ifelse(!is.na(geo_lat), "geo", sp_res)) 

frq(tweets_clean_2019_07_22, sp_res, sort.frq = "desc")
```

### Twitter coordinates

```{r}
geo2_sp <- readRDS(file = "./data/Twitter/clean/geo2_sp.rds")
```

There are `r sum(geo2_sp$freq)` tweets in `r nrow(geo2_sp)` distinct locations having precise coordinates (*including update above*). 

Here a sample of locations with at least `100` tweets:

```{r}
subset(geo2_sp, freq > 99) %>% 
  tm_shape() + 
  tm_bubbles(size = "freq", col = "red", alpha = 0.25, border.lwd = NA)
```

They clearly reflect `bbox` used for search and still include tweets located, for instance, in Indonesia and on the ocean (sic!). 

```{r include=FALSE}
rm(geo2_sp)
```


#### Tweets inside Oz

```{r}
geo2_inside_sp <- readRDS(file = "./data/Twitter/clean/geo2_inside_sp.rds")

tweets_geo2_inside <- left_join(geo2_inside_sp@data, tweets_clean_2019_07_22) %>% 
  ungroup() %>%
  select(-freq) %>% 
  filter(!is.na(created_at))

# tm_shape(tweets_geo2_inside) + tm_dots(col = "red")

# ggplot(tweets_geo2_inside) + 
#   geom_histogram(aes(created_at)) + 
#   scale_x_datetime()
```

From tweets with coordinates mentioned above, there are `r comma(sum(geo2_inside_sp$freq))` tweets in `r comma(nrow(geo2_inside_sp))` distinct locations having coordinates and overlaying shape of Austrlia with 0.5 degree buffer around it (*to capture islands, reef, imprecise coords*). 

Note: map too large for interactive.

```{r include=FALSE}
rm(geo2_inside_sp)
```

#### Tweets outside  Oz

```{r}
geo2_outside_sp <- readRDS(file = "./data/Twitter/clean/geo2_outside_sp.rds")

tweets_geo2_outside <- left_join(geo2_outside_sp@data, tweets_clean_2019_07_22) %>% 
  filter(!is.na(created_at))

tweets_clean_2019_07_22 <- anti_join(tweets_clean_2019_07_22, 
                                     select(geo2_outside_sp@data, lon, lat)) 

geo2_outside_places <- tweets_geo2_outside %>% 
  select(place_full_name) %>% 
  group_by(place_full_name) %>% 
  summarise(freq = n()) %>% 
  arrange(desc(freq))
```

From tweets with coordinates mentioned above, there are `r comma(sum(geo2_outside_sp$freq))` tweets in `r comma(nrow(tweets_geo2_outside))` distinct locations having coordinates and **not** overlaying shape of Austrlia with 0.5 degree buffer around it. 


```{r}
tm_shape(geo2_outside_sp) + 
  tm_bubbles(size = "freq", col = "red")
```

Seems mostly fine with few exceptions:

```{r}
geo2_outside_places %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

These tweets have now been excluded. Some of the them could be rescued with a bit of cleaning? It seems that some of the place names could be used to bring them back? 

```{r include=FALSE}
rm(geo2_outside_sp, geo2_outside_places, tweets_geo2_outside)
gc()
```


### Geocoding `place_full_name`

```{r}
place_full_name <- tweets_clean_2019_07_22 %>% 
  filter(are_na(lat)) %>% 
  group_by(place_full_name) %>% 
  summarise(freq = n()) %>% 
  arrange(desc(freq)) 

# descr(place_full_name$freq) 

```

Large chunk of tweets - `r comma(sum(place_full_name$freq))` (or `r percent(sum(place_full_name$freq) / nrow(tweets_clean_2019_07_22))`) to be specific, have **no Twitter coordinates**. They are however labeled with `place_full_name`. Here are top 30 most frequent names, with corresponding number of tweets missing coordinates: 

```{r}
place_full_name %>% 
  slice(1:30) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

There are `r comma(nrow(place_full_name))` distinct locations mentioned with various quality and geographical precision. [Google Maps Geocoding API](https://developers.google.com/maps/documentation/geocoding/intro) (via package `ggmap`) was used to get coordinates for these places. 

No all the place names are sufficient to determine location precisely. Examples of few names that are imprecise.. 

```{r}
errors <- c("Banjo Paterson Park", "Kookaburra Park", "Sportscare", "Park", "Outback")

geocoded_places_asgs %>%
  filter(place_full_name %in% errors)
```

and few where even more informative name doesnt point to one place...

![Banjo Paterson Park.](banjo.PNG)


Different place names offer different precision of link to official geography. 

```{r}
geocoded_places_asgs <- readRDS("./data/Twitter/raw/geocoded_places_asgs.rds") 

frq(geocoded_places_asgs$max_sp_res)
```

So far only SA2 level geocodes are used.

```{r include=FALSE}
geocoded_places_asgs %<>% 
  filter(max_sp_res == "SA2") %>% 
  select(-point_SA2) %>% 
  rename(gog_lat = lat,
         gog_lon = lon)

tweets_clean_2019_07_22 %<>% 
  left_join(geocoded_places_asgs) %>% 
  mutate(sp_res = ifelse(is.na(lat) & !is.na(gog_lat), "approx", sp_res)) %>% 
  mutate(lat = ifelse(is.na(lat) & !is.na(gog_lat), gog_lat, lat)) %>% 
  mutate(lon = ifelse(is.na(lon) & !is.na(gog_lon), gog_lon, lon)) %>% 
  select(-max_sp_res) %>% 
  mutate(sp_res = na_if(sp_res, ""))

# temp <- 
#   tweets_clean_2019_07_22 %>% 
#   filter(is.na(lat)) %>% 
#   group_by(place_full_name) %>% 
#   summarise(freq = n()) %>% 
#   arrange(desc(freq))

rm(geocoded_places_asgs, place_full_name)
```

After merging these coordinates, here is the distribution of geo usability of tweets:

```{r}
# frq(tweets_clean_2019_07_22, max_sp_res)
frq(tweets_clean_2019_07_22, sp_res)

# tweets_clean_2019_07_22 %>% 
#   filter(place_full_name == "Recherche, Tasmania") %>% 
#   select(place_full_name, sp_res)
```

## Timezones

### Finding 

![Time zones.](./docs/tzmap-australia-nz.png)

Source: https://www.timeanddate.com/time/australia/time-zones-background.html

Using package `lutz` from https://github.com/ateucher/lutz time zones were obtained for every location for locations with Twitter (precise) coordinates and geocoded place names. `created_at` variable is still in UTC time though since there is no no easy vectorised way to update timestamp.

```{r, include=FALSE}
# format(anytime("2018-08-28 07:32:04", tz = "UTC", asUTC = TRUE), tz="Australia/Brisbane", usetz = TRUE)
# format(anytime("2018-08-28 07:32:04", tz = "UTC", asUTC = TRUE), tz="Australia/Perth", usetz = TRUE)

# with_tz(anytime("2018-08-28 07:32:04", tz = "UTC", asUTC = TRUE), tzone = "Australia/Brisbane")
# with_tz(anytime("2018-08-28 07:32:04", tz = "UTC", asUTC = TRUE), tzone = "Australia/Perth")

tweets_tz <- tweets_clean_2019_07_22 %>% 
  filter(!is.na(lat)) %>% 
  filter(!is.na(lon)) %>% 
  group_by(lat, lon) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) %>% 
  select(-count) %>% 
  ungroup()
```

`r comma(nrow(tweets_tz))` locations (inside Oz) had their time zone defined. 

```{r}
tweets_tz$tz <- tz_lookup_coords(tweets_tz$lat, tweets_tz$lon, method = "accurate")
sjmisc::frq(tweets_tz, tz, sort.frq = "desc", show.na = TRUE) 

tweets_clean_2019_07_22 <- left_join(tweets_clean_2019_07_22, tweets_tz) 
rm(tweets_tz)
sjmisc::frq(tweets_clean_2019_07_22, tz, sort.frq = "desc", show.na = TRUE)
```


### Updating

**TBD**

```{r}
# # convert time zone based on lat lon
# tweets_geo2_inside$created_at_tz <- tweets_geo2_inside$created_at
# 
# for (i in 1:nrow(tweets_geo2_inside)) {
#   tweets_geo2_inside$created_at_tz[i] <- anytime(as.character(tweets_geo2_inside$created_at[i]),
#                                             asUTC = FALSE, tz = as.character(tweets_geo2_inside$tz[i]))
#   tweets_geo2_inside$created_at_tz[i] <- force_tz(tweets_geo2_inside$created_at_tz[i],
#                                           tzone = as.character(tweets_geo2_inside$tz[i]))
# }
# 
# table(tz(tweets_geo2_inside2_inside$created_at))
# table(tweets_geo2_inside2_inside$tz)
# table(tz(tweets_geo2_inside2_inside$created_at_tz))

# tweets_geo2_inside2_inside$temp <- tweets_geo2_inside2_inside$created_at_tz - tweets_geo2_inside2_inside$created_at
# table(tweets_geo2_inside2_inside$temp)
# tweets_geo2_inside2_inside$temp <- NULL

# # extracting parts of date - some as numbers some as strings!
# tweets_geo2_inside %<>% 
#   mutate(created_at_hod = as.numeric(format(created_at_tz, "%H"))) %>% # hour
#   mutate(created_at_hod_notz = as.numeric(format(created_at, "%H"))) 

# # fake timestamp - original time and the same date
# tweets_geo2_inside$created_at_time_2 <- anytime(paste("2018-01-01", # same date for all tweets
#                                               format(tweets_geo2_inside$created_at_tz, format="%T")) # original time of tweet
# )

# summary(tweets_geo2_inside$created_at_tz)
# summary(tweets_geo2_inside$created_at_time_2)
```


```{r eval=FALSE, include=FALSE}
# Notice the difference of updated time zone for `r nrow(tweets_geo)` tweets with coords.

# ggplot(tweets_geo2_inside, aes(created_at_hod_notz)) + 
#   geom_bar() 
# 
# ggplot(tweets_geo2_inside, aes(created_at_hod)) + 
#   geom_bar() 
# 
# tweets_geo %<>% 
#   select(-created_at_hod_notz, -user_utc_offset, -user_time_zone)

# tweets_geo %<>% 
#   select(-user_utc_offset, -user_time_zone)
```


## Finding bots

Note: only geolocated tweetswere used here.

### Using very frequent tweets

```{r}
temp1 <- tweets_geo2_inside %>% 
  dplyr::arrange(UID, created_at) %>% 
  dplyr::group_by(UID) %>%
  dplyr::mutate(tdiff = created_at - dplyr::lag(created_at)) %>% 
  dplyr::select(UID, user_id_str, created_at, tdiff, 
                lat, lon, user_description, text) %>% 
  ungroup() 

temp2 <- temp1 %>% 
  dplyr::filter(tdiff <= 1) %>% 
  dplyr::group_by(UID) %>% 
  dplyr::summarise(htime = n(), 
                   first(user_id_str), first(user_description)) %>% 
  dplyr::filter(htime >= 9) %>%
  ungroup() %>% 
  arrange(desc(htime))

temp3 <- temp2 %>% 
  left_join(tweets_geo2_inside) %>% 
  arrange(desc(htime), UID, created_at) %>% 
  select(htime, UID, user_id_str, created_at, user_description, text, everything()) 
```


'High frequency' tweets were found looking at the timestamp of consecutive tweets for each user. If difference of time between tweets was equal or less than a second and user created 9 or more such tweets, all tweets of this user were excluded.

`r comma(nrow(temp3))` tweets of `r comma(nrow(temp2))` users were excluded using this method.

```{r}
temp2 %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

```{r include=FALSE}
tweets_geo2_inside %<>% 
  anti_join(select(temp3, UID))

rm(temp1, temp2, temp3)
gc()
```


### Using tweets with unusual speed

```{r message=FALSE, warning=FALSE, include=FALSE}
temp1 <- tweets_geo2_inside %>% 
  dplyr::arrange(UID, created_at) %>% 
  dplyr::group_by(UID) %>%
  dplyr::mutate(tdiff = created_at - dplyr::lag(created_at),
                lat_lag = dplyr::lag(lat),
                lon_lag = dplyr::lag(lon),
                # latdiff = lat - dplyr::lag(lat),
                # londiff = lon - dplyr::lag(lon),
                days = max(as.Date(created_at)) - min(as.Date(created_at))) %>% 
  dplyr::select(UID, user_id_str, created_at, tdiff, days, 
                lat, lon, lat_lag, lon_lag, # latdiff, londiff,
                user_description, text) %>% 
  ungroup() 

p_load(geosphere)

temp1 %<>% 
  mutate(dvsp = distVincentyEllipsoid(cbind(lon, lat), cbind(lon_lag, lat_lag)))

# temp1 %<>% 
#   rowwise() %>% 
#   mutate(dvsp = distVincentyEllipsoid(c(lon, lat), c(lon_lag, lat_lag))) %>% 
#   ungroup() 

# temp1$dvsp <- NA
# for (i in 1:nrow(temp1)) {
#   if(!is.na(temp1$lon_lag[i])) { 
#     temp1$dvsp[i] <- distVincentyEllipsoid(c(temp1$lon[i], temp1$lat[i]), 
#                                            c(temp1$lon_lag[i], temp1$lat_lag[i]))
#   }
# }

p_unload(geosphere)

temp1$dvsp <- as.integer(temp1$dvsp)
temp1$speed <- as.integer(temp1$dvsp / as.numeric(temp1$tdiff))

temp2 <- temp1 %>% 
  dplyr::filter(speed > 10000 & !is.na(speed)) %>% 
  dplyr::group_by(UID) %>% 
  dplyr::summarise(hspeed = n(), first(user_description)) %>% 
  arrange(desc(hspeed)) %>% 
  ungroup()

temp3 <- temp2 %>% 
  select(UID, hspeed) %>% 
  left_join(tweets_geo2_inside) %>% 
  arrange(desc(hspeed), UID, created_at) %>% 
  dplyr::select(hspeed, UID, user_id_str, created_at, 
                lat, lon, 
                user_description, text) 
```


'High speed' tweets were found looking at the time difference and distance between consecutive tweets for each user (to derive speed of movement). If speed had unrealistic values (here testing `10,000` m/s), all tweets of this user were excluded.

`r comma(nrow(temp3))` tweets of `r comma(nrow(temp2))` users were excluded using this method.

```{r}
temp2 %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

```{r eval=FALSE, include=FALSE}
temp <- temp1 %>% 
  filter(UID == 16169) 

temp %>% 
  group_by(lon, lat) %>% 
  summarise(freq = n()) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = FALSE) %>% 
  tm_shape() + tm_bubbles(size = "freq", col = "red")

temp %>% 
  dplyr::filter(speed < 200 & !is.na(speed)) %>% 
  group_by(lon, lat) %>% 
  summarise(freq = n()) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = FALSE) %>% 
  tm_shape() + tm_bubbles(size = "freq", col = "red")

temp %>% 
  dplyr::filter(speed >= 200 & !is.na(speed)) %>% 
  group_by(lon, lat) %>% 
  summarise(freq = n()) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = FALSE) %>% 
  tm_shape() + tm_bubbles(size = "freq", col = "red")



temp <- temp1 %>% 
  filter(UID == 174582) 

temp %>% 
  group_by(lon, lat) %>% 
  summarise(freq = n()) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = FALSE) %>% 
  tm_shape() + tm_bubbles(size = "freq", col = "red")

temp %>% 
  dplyr::filter(speed < 200 & !is.na(speed)) %>% 
  group_by(lon, lat) %>% 
  summarise(freq = n()) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = FALSE) %>% 
  tm_shape() + tm_bubbles(size = "freq", col = "red")

temp %>% 
  dplyr::filter(speed >= 200 & !is.na(speed)) %>% 
  group_by(lon, lat) %>% 
  summarise(freq = n()) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = FALSE) %>% 
  tm_shape() + tm_bubbles(size = "freq", col = "red")
```

```{r eval=FALSE, include=FALSE}
tweets_geo2_inside %<>% 
  anti_join(select(temp3, UID))

rm(temp1, temp2, temp3)
gc()
```

Less conservative approach would be to removeonly particular tweets?

```{r, include=FALSE}
tweets_clean_2019_07_22 %<>% 
  select(id_str, user_id_str, user_screen_name, UID, 
         created_at, 
         lat, lon, 
         tz, 
         # orig_lat, orig_lon, geo_lat, geo_lon, gog_lat, gog_lon,
         starts_with("place"), -place_id, -place_bounding_box, -timestamp_ms,
         everything(), source) %>% 
  arrange(created_at, UID)

saveRDS(tweets_clean_2019_07_22, file = "./data/Twitter/clean/tweets_clean_2019_07_22.rds")

```


<!-- ------------------------------------------------------------ --> 
<!-- ------------------------------------------------------------ --> 

# Results

`tweets_clean_2019_07_22` dataset has `r nrow(tweets_clean_2019_07_22)` rows and `r ncol(tweets_clean_2019_07_22)` columns.

## Structure

```{r, results='asis'}
skim(tweets_clean_2019_07_22) %>% 
  skimr::kable()
```

## Missing data

```{r}
gg_miss_var(tweets_clean_2019_07_22, show_pct = TRUE)
```

## Time

It's UTC for everybody!

```{r}
summary(tweets_clean_2019_07_22$created_at)
```

All tweets (There is a gap?)

```{r}
tweets_clean_2019_07_22 %>% 
  # slice(1:10) %>% 
  select(created_at) %>% 
  mutate(date = as.Date(created_at)) %>% 
  ggplot() +
  geom_histogram(aes(date), binwidth = 1) +
  scale_x_date(date_breaks = "1 month", date_minor_breaks = "1 week") + 
  theme_minimal()
```

Geolocated only

```{r}
tweets_clean_2019_07_22 %>% 
  filter(!is.na(lat)) %>% 
  select(created_at) %>% 
  mutate(date = as.Date(created_at)) %>% 
  ggplot() +
  geom_histogram(aes(date), binwidth = 1) +
  scale_x_date(date_breaks = "1 month", date_minor_breaks = "1 week") + 
  theme_minimal()
```

<!-- ------------------------------------------------------------ --> 
# Aggregates from Alex

Processed data including:

- `tweets` - total number
- `distinct_users` - distinct users over the month
- `avg_distinct_daily_users` - average number of distinct users from each day


```{r}
twitter_ABS_month_SA3 <- readRDS("./data/Twitter/clean/twitter_ABS_month_SA3.rds")

twitter_ABS_month_SA3 %>% 
  ggplot(aes(x = month, y = distinct_users, group = SA3_NAME_2016)) +
  geom_line() + 
  theme_light() + 
  ylab("Distinct Twitter users") + xlab("")
```

